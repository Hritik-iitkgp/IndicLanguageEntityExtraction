{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6851440,"sourceType":"datasetVersion","datasetId":3938247},{"sourceId":6851842,"sourceType":"datasetVersion","datasetId":3938468},{"sourceId":6883472,"sourceType":"datasetVersion","datasetId":3954811}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"###  Given a query from an online healthcare platform, identify the intent class the query belongs to.\n- Using Different pretrained model\n- for multiple language by directly using the source language as well converting them to language","metadata":{}},{"cell_type":"markdown","source":"## Intent Recognition in English Queries Using BioClinicalBERT as pre trained model \n- On Two Dataset of indic-health-demo \n    - ihiqid-1mg\n    - ihqid-webmd","metadata":{}},{"cell_type":"code","source":"# Read the Dataset\nimport pandas as pd\ndf=pd.read_csv('/kaggle/input/ihqid-1mg/train.csv')\ndf3=pd.read_csv('/kaggle/input/ihqid-1mg1/test.csv')\ny_train=df['Manual_Intent']\nX_train=df['question_english']\ny_test=df3['Manual_Intent']\nX_test=df3['question_english']","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:03:35.874176Z","iopub.execute_input":"2023-12-09T05:03:35.874656Z","iopub.status.idle":"2023-12-09T05:03:36.394309Z","shell.execute_reply.started":"2023-12-09T05:03:35.874621Z","shell.execute_reply":"2023-12-09T05:03:36.393246Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Trainer, TrainingArguments\n\n\n# Encode the intent labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y_train)\ny_t_encoded = label_encoder.fit_transform(y_test)\n\n# Load the RoBERTa tokenizer and model\nmodel_name = \"emilyalsentzer/Bio_ClinicalBERT\"  # You can choose a specific RoBERTa model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n\n# Tokenize and encode the training and testing data\nclass IntentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.encodings = tokenizer(\n            texts,\n            truncation=True,\n            padding='max_length',\n            max_length=max_length,\n            return_tensors='pt'\n        )\n        self.labels = torch.tensor(labels)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = self.labels[idx]\n        return item\n#print(y_train)\ntrain_dataset = IntentDataset(X_train.tolist(), y_encoded, tokenizer, max_length=64)\ntest_dataset = IntentDataset(X_test.tolist(), y_t_encoded, tokenizer, max_length=64)\n\n# Define training arguments and a trainer\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"steps\",\n    eval_steps=40,\n    output_dir=\"/kaggle/working\",\n    num_train_epochs=6,\n    load_best_model_at_end=True,\n    save_steps=1000,\n    save_total_limit=2,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(-1) == p.label_ids).mean()},\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Evaluate the fine-tuned model on the test set\nresults = trainer.evaluate()\n\n# Print the evaluation results\nprint(results)\n\n# Generate predictions for the test set\ny_pred_intent = trainer.predict(test_dataset).predictions.argmax(axis=1)\n\n# Decode the encoded labels\ny_pred_intent_decoded = label_encoder.inverse_transform(y_pred_intent)\n# Print the classification report\nreport = classification_report(y_test, y_pred_intent_decoded, target_names=label_encoder.classes_)\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:03:36.396415Z","iopub.execute_input":"2023-12-09T05:03:36.396763Z","iopub.status.idle":"2023-12-09T05:18:19.828040Z","shell.execute_reply.started":"2023-12-09T05:03:36.396732Z","shell.execute_reply":"2023-12-09T05:18:19.827123Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2a64279b0674804b3fdcb5639868eba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b0935a9b40c4061b330ccec41421e77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af791d2fef494cb985470c1df2745bf9"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231209_050517-se2zmc2p</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hritik-j/huggingface/runs/se2zmc2p' target=\"_blank\">splendid-planet-3</a></strong> to <a href='https://wandb.ai/hritik-j/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hritik-j/huggingface' target=\"_blank\">https://wandb.ai/hritik-j/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hritik-j/huggingface/runs/se2zmc2p' target=\"_blank\">https://wandb.ai/hritik-j/huggingface/runs/se2zmc2p</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [234/234 12:03, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>40</td>\n      <td>No log</td>\n      <td>0.739988</td>\n      <td>0.687500</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>No log</td>\n      <td>0.683072</td>\n      <td>0.758929</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>No log</td>\n      <td>0.755159</td>\n      <td>0.803571</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>No log</td>\n      <td>0.860728</td>\n      <td>0.812500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>No log</td>\n      <td>0.920706</td>\n      <td>0.794643</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.9402279257774353, 'eval_accuracy': 0.8125, 'eval_runtime': 9.8308, 'eval_samples_per_second': 11.393, 'eval_steps_per_second': 1.424, 'epoch': 6.0}\n              precision    recall  f1-score   support\n\n     disease       0.71      0.83      0.77        24\n        drug       0.91      0.96      0.94        54\n       other       0.50      0.15      0.24        13\n   treatment       0.74      0.81      0.77        21\n\n    accuracy                           0.81       112\n   macro avg       0.72      0.69      0.68       112\nweighted avg       0.79      0.81      0.79       112\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Intent Reconginition on ihqid_webmd Dataset","metadata":{}},{"cell_type":"code","source":"webmd_train=pd.read_csv('/kaggle/input/ihqid-webmd/train.csv')\nwebmd_test=pd.read_csv('/kaggle/input/ihqid-webmd/test.csv')\n# extract only reuired english columns\n#df1=webmd_train[['question_english','disease_english','drug_english','treatment_english']]\n#df2=webmd_test[['question_english','disease_english','drug_english','treatment_english']]\n#df1['intent'] = df1[['disease_english','drug_english','treatment_english']].notna().idxmax(axis=1)\n#df2['intent'] = df2[['disease_english','drug_english','treatment_english']].notna().idxmax(axis=1)\ny_train=webmd_train['Manual_Intent']\nX_train=webmd_train['question_english']\ny_test=webmd_test['Manual_Intent']\nX_test=webmd_test['question_english']","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:18:19.829819Z","iopub.execute_input":"2023-12-09T05:18:19.830568Z","iopub.status.idle":"2023-12-09T05:18:19.925408Z","shell.execute_reply.started":"2023-12-09T05:18:19.830532Z","shell.execute_reply":"2023-12-09T05:18:19.924352Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Trainer, TrainingArguments\n\n\n# Encode the intent labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y_train)\ny_t_encoded = label_encoder.fit_transform(y_test)\n\n# Load the RoBERTa tokenizer and model\nmodel_name = \"emilyalsentzer/Bio_ClinicalBERT\"  # You can choose a specific RoBERTa model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n\n# Tokenize and encode the training and testing data\nclass IntentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.encodings = tokenizer(\n            texts,\n            truncation=True,\n            padding='max_length',\n            max_length=max_length,\n            return_tensors='pt'\n        )\n        self.labels = torch.tensor(labels)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = self.labels[idx]\n        return item\nprint(y_train)\ntrain_dataset = IntentDataset(X_train.tolist(), y_encoded, tokenizer, max_length=64)\ntest_dataset = IntentDataset(X_test.tolist(), y_t_encoded, tokenizer, max_length=64)\n\n# Define training arguments and a trainer\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"steps\",\n    eval_steps=40,\n    output_dir=\"/kaggle/working\",\n    num_train_epochs=3,\n    load_best_model_at_end=True,\n    save_steps=1000,\n    save_total_limit=2,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(-1) == p.label_ids).mean()},\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Evaluate the fine-tuned model on the test set\nresults = trainer.evaluate()\n\n# Print the evaluation results\nprint(results)\n\n# Generate predictions for the test set\ny_pred_intent = trainer.predict(test_dataset).predictions.argmax(axis=1)\n\n# Decode the encoded labels\ny_pred_intent_decoded = label_encoder.inverse_transform(y_pred_intent)\n# Print the classification report\nreport = classification_report(y_test, y_pred_intent_decoded, target_names=label_encoder.classes_)\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:18:19.928293Z","iopub.execute_input":"2023-12-09T05:18:19.929077Z","iopub.status.idle":"2023-12-09T05:34:23.540685Z","shell.execute_reply.started":"2023-12-09T05:18:19.929018Z","shell.execute_reply":"2023-12-09T05:34:23.539478Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"0                drug\n1               other\n2                drug\n3             disease\n4             disease\n            ...      \n715             other\n716    treatment plan\n717    treatment plan\n718    treatment plan\n719    treatment plan\nName: Manual_Intent, Length: 720, dtype: object\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [270/270 15:13, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>40</td>\n      <td>No log</td>\n      <td>1.183671</td>\n      <td>0.560166</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>No log</td>\n      <td>0.818702</td>\n      <td>0.721992</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>No log</td>\n      <td>0.769937</td>\n      <td>0.717842</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>No log</td>\n      <td>0.784351</td>\n      <td>0.738589</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>No log</td>\n      <td>0.746537</td>\n      <td>0.751037</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>No log</td>\n      <td>0.746125</td>\n      <td>0.763485</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.7441739439964294, 'eval_accuracy': 0.7717842323651453, 'eval_runtime': 21.5213, 'eval_samples_per_second': 11.198, 'eval_steps_per_second': 1.44, 'epoch': 3.0}\n                precision    recall  f1-score   support\n\n       disease       0.78      0.84      0.81        76\n          drug       0.75      0.81      0.78        53\n         other       0.79      0.77      0.78        73\ntreatment plan       0.74      0.59      0.66        39\n\n      accuracy                           0.77       241\n     macro avg       0.77      0.75      0.76       241\n  weighted avg       0.77      0.77      0.77       241\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Intent Recognition in English Queries Using RoBERTa as pre trained model ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv('/kaggle/input/ihqid-1mg/train.csv')\ndf3=pd.read_csv('/kaggle/input/ihqid-1mg1/test.csv')\ny_train=df['Manual_Intent']\nX_train=df['question_english']\ny_test=df3['Manual_Intent']\nX_test=df3['question_english']","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:34:23.542151Z","iopub.execute_input":"2023-12-09T05:34:23.543231Z","iopub.status.idle":"2023-12-09T05:34:23.589855Z","shell.execute_reply.started":"2023-12-09T05:34:23.543187Z","shell.execute_reply":"2023-12-09T05:34:23.588433Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Trainer, TrainingArguments\n\n\n# Encode the intent labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y_train)\ny_t_encoded = label_encoder.fit_transform(y_test)\n\n# Load the RoBERTa tokenizer and model\nmodel_name = \"roberta-base\"  # You can choose a specific RoBERTa model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n\n# Tokenize and encode the training and testing data\nclass IntentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.encodings = tokenizer(\n            texts,\n            truncation=True,\n            padding='max_length',\n            max_length=max_length,\n            return_tensors='pt'\n        )\n        self.labels = torch.tensor(labels)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = self.labels[idx]\n        return item\ntrain_dataset = IntentDataset(X_train.tolist(), y_encoded, tokenizer, max_length=64)\ntest_dataset = IntentDataset(X_test.tolist(), y_t_encoded, tokenizer, max_length=64)\n\n# Define training arguments and a trainer\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"steps\",\n    eval_steps=60,\n    output_dir=\"/kaggle/working\",\n    num_train_epochs=3,\n    load_best_model_at_end=True,\n    save_steps=600,\n    save_total_limit=2,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(-1) == p.label_ids).mean()},\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Evaluate the fine-tuned model on the test set\nresults = trainer.evaluate()\n\n# Print the evaluation results\nprint(results)\n\n# Generate predictions for the test set\ny_pred_intent = trainer.predict(test_dataset).predictions.argmax(axis=1)\n\n# Decode the encoded labels\ny_pred_intent_decoded = label_encoder.inverse_transform(y_pred_intent)\n# Print the classification report\nreport = classification_report(y_test, y_pred_intent_decoded, target_names=label_encoder.classes_)\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:34:23.591291Z","iopub.execute_input":"2023-12-09T05:34:23.592140Z","iopub.status.idle":"2023-12-09T05:41:08.305197Z","shell.execute_reply.started":"2023-12-09T05:34:23.592101Z","shell.execute_reply":"2023-12-09T05:41:08.303912Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"337f2a01208145ffb0769082acd52724"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9c74cf226ba4319863a1346d0345ebd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c070d1bf01c4481a5e2e54cc68f43df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddff1764406341c28c80c3c74a102d89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"080c4f40ff534948ab18253ef8d96b11"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='117' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [117/117 05:59, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>60</td>\n      <td>No log</td>\n      <td>0.884299</td>\n      <td>0.705357</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.7832251191139221, 'eval_accuracy': 0.7589285714285714, 'eval_runtime': 10.0008, 'eval_samples_per_second': 11.199, 'eval_steps_per_second': 1.4, 'epoch': 3.0}\n              precision    recall  f1-score   support\n\n     disease       0.59      0.92      0.72        24\n        drug       0.89      1.00      0.94        54\n       other       0.33      0.15      0.21        13\n   treatment       0.88      0.33      0.48        21\n\n    accuracy                           0.76       112\n   macro avg       0.67      0.60      0.59       112\nweighted avg       0.76      0.76      0.72       112\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Intent Reconginition on ihqid_webmd Dataset ","metadata":{}},{"cell_type":"code","source":"webmd_train=pd.read_csv('/kaggle/input/ihqid-webmd/train.csv')\nwebmd_test=pd.read_csv('/kaggle/input/ihqid-webmd/test.csv')\ny_train=webmd_train['Manual_Intent']\nX_train=webmd_train['question_english']\ny_test=webmd_test['Manual_Intent']\nX_test=webmd_test['question_english']","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:41:08.306671Z","iopub.execute_input":"2023-12-09T05:41:08.307793Z","iopub.status.idle":"2023-12-09T05:41:08.366747Z","shell.execute_reply.started":"2023-12-09T05:41:08.307755Z","shell.execute_reply":"2023-12-09T05:41:08.365831Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Trainer, TrainingArguments\n\n\n# Encode the intent labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y_train)\ny_t_encoded = label_encoder.fit_transform(y_test)\n\n# Load the RoBERTa tokenizer and model\nmodel_name = \"roberta-base\"  # You can choose a specific RoBERTa model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n\n# Tokenize and encode the training and testing data\nclass IntentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.encodings = tokenizer(\n            texts,\n            truncation=True,\n            padding='max_length',\n            max_length=max_length,\n            return_tensors='pt'\n        )\n        self.labels = torch.tensor(labels)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = self.labels[idx]\n        return item\ntrain_dataset = IntentDataset(X_train.tolist(), y_encoded, tokenizer, max_length=64)\ntest_dataset = IntentDataset(X_test.tolist(), y_t_encoded, tokenizer, max_length=64)\n\n# Define training arguments and a trainer\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"steps\",\n    eval_steps=90,\n    output_dir=\"/kaggle/working\",\n    num_train_epochs=2,\n    load_best_model_at_end=True,\n    save_steps=900,\n    save_total_limit=2,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(-1) == p.label_ids).mean()},\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Evaluate the fine-tuned model on the test set\nresults = trainer.evaluate()\n\n# Print the evaluation results\nprint(results)\n\n# Generate predictions for the test set\ny_pred_intent = trainer.predict(test_dataset).predictions.argmax(axis=1)\n\n# Decode the encoded labels\ny_pred_intent_decoded = label_encoder.inverse_transform(y_pred_intent)\n# Print the classification report\nreport = classification_report(y_test, y_pred_intent_decoded, target_names=label_encoder.classes_)\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:41:08.368456Z","iopub.execute_input":"2023-12-09T05:41:08.369141Z","iopub.status.idle":"2023-12-09T05:51:46.655922Z","shell.execute_reply.started":"2023-12-09T05:41:08.369099Z","shell.execute_reply":"2023-12-09T05:51:46.654643Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [180/180 09:47, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>90</td>\n      <td>No log</td>\n      <td>0.808182</td>\n      <td>0.742739</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>No log</td>\n      <td>0.721608</td>\n      <td>0.771784</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.72160804271698, 'eval_accuracy': 0.7717842323651453, 'eval_runtime': 21.8293, 'eval_samples_per_second': 11.04, 'eval_steps_per_second': 1.42, 'epoch': 2.0}\n                precision    recall  f1-score   support\n\n       disease       0.83      0.71      0.77        76\n          drug       0.78      0.89      0.83        53\n         other       0.75      0.77      0.76        73\ntreatment plan       0.71      0.74      0.72        39\n\n      accuracy                           0.77       241\n     macro avg       0.77      0.78      0.77       241\n  weighted avg       0.77      0.77      0.77       241\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Intent Recognition in English Queries Using SVM","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\ndf=pd.read_csv('/kaggle/input/ihqid-1mg/train.csv')\ndf3=pd.read_csv('/kaggle/input/ihqid-1mg1/test.csv')\ny_train=df['Manual_Intent']\nX_train=df['question_english']\ny_test=df3['Manual_Intent']\nX_test=df3['question_english']\n\n# Create a TF-IDF vectorizer\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\nX_tfidf = tfidf_vectorizer.fit_transform(X_train)\n\n# Train an SVM model for the intent\nsvm_model = SVC()\nsvm_model.fit(X_tfidf, y_train)\n\n# Transform X_test using the same TF-IDF vectorizer\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\n# Predict the \"drug\" intent for the test set\ny_pred_drug_intent = svm_model.predict(X_test_tfidf)\n\n# Print the classification report\nreport = classification_report(y_test, y_pred_drug_intent, target_names=['drug', 'disease','treatment','other'])\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:51:46.660456Z","iopub.execute_input":"2023-12-09T05:51:46.660890Z","iopub.status.idle":"2023-12-09T05:51:46.869125Z","shell.execute_reply.started":"2023-12-09T05:51:46.660858Z","shell.execute_reply":"2023-12-09T05:51:46.868375Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n        drug       0.47      0.83      0.60        24\n     disease       0.81      1.00      0.89        54\n   treatment       0.50      0.08      0.13        13\n       other       0.00      0.00      0.00        21\n\n    accuracy                           0.67       112\n   macro avg       0.44      0.48      0.41       112\nweighted avg       0.55      0.67      0.57       112\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Intent Reconginition on ihqid_webmd Dataset using svm","metadata":{}},{"cell_type":"code","source":"webmd_train=pd.read_csv('/kaggle/input/ihqid-webmd/train.csv')\nwebmd_test=pd.read_csv('/kaggle/input/ihqid-webmd/test.csv')\n# extract only reuired english columns\n#df1=webmd_train[['question_english','disease_english','drug_english','treatment_english']]\n#df2=webmd_test[['question_english','disease_english','drug_english','treatment_english']]\n#df1['intent'] = df1[['disease_english','drug_english','treatment_english']].notna().idxmax(axis=1)\n#df2['intent'] = df2[['disease_english','drug_english','treatment_english']].notna().idxmax(axis=1)\ny_train=webmd_train['Manual_Intent']\nX_train=webmd_train['question_english']\ny_test=webmd_test['Manual_Intent']\nX_test=webmd_test['question_english']","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:58:06.714882Z","iopub.execute_input":"2023-12-09T05:58:06.715407Z","iopub.status.idle":"2023-12-09T05:58:06.781153Z","shell.execute_reply.started":"2023-12-09T05:58:06.715372Z","shell.execute_reply":"2023-12-09T05:58:06.779896Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#train and report on the ihiqid_webmd data\n# Create a TF-IDF vectorizer\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\nX_tfidf = tfidf_vectorizer.fit_transform(X_train)\n\n# Train an SVM model for the \"drug\" intent\nsvm_model = SVC()\nsvm_model.fit(X_tfidf, y_train)\n\n# Transform X_test using the same TF-IDF vectorizer\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\n# Predict the \"drug\" intent for the test set\ny_pred_drug_intent = svm_model.predict(X_test_tfidf)\n\n# Print the classification report\nreport = classification_report(y_test, y_pred_drug_intent, target_names=['drug', 'disease','treatment','other'])\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T05:58:10.143200Z","iopub.execute_input":"2023-12-09T05:58:10.144320Z","iopub.status.idle":"2023-12-09T05:58:10.269258Z","shell.execute_reply.started":"2023-12-09T05:58:10.144276Z","shell.execute_reply":"2023-12-09T05:58:10.268437Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n        drug       0.59      0.82      0.69        76\n     disease       0.62      0.57      0.59        53\n   treatment       0.64      0.64      0.64        73\n       other       0.86      0.31      0.45        39\n\n    accuracy                           0.63       241\n   macro avg       0.68      0.58      0.59       241\nweighted avg       0.65      0.63      0.61       241\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Intent Recognition in Indic language Queries Using RoBERTa as pre trained model by translating it to English","metadata":{}},{"cell_type":"code","source":"! pip install deep-translator","metadata":{"execution":{"iopub.status.busy":"2023-12-09T06:01:45.344893Z","iopub.execute_input":"2023-12-09T06:01:45.345789Z","iopub.status.idle":"2023-12-09T06:01:57.616122Z","shell.execute_reply.started":"2023-12-09T06:01:45.345744Z","shell.execute_reply":"2023-12-09T06:01:57.614491Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting deep-translator\n  Obtaining dependency information for deep-translator from https://files.pythonhosted.org/packages/38/3f/61a8ef73236dbea83a1a063a8af2f8e1e41a0df64f122233938391d0f175/deep_translator-1.11.4-py3-none-any.whl.metadata\n  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /opt/conda/lib/python3.10/site-packages (from deep-translator) (4.12.2)\nRequirement already satisfied: requests<3.0.0,>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from deep-translator) (2.31.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.3.2.post1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2023.7.22)\nDownloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: deep-translator\nSuccessfully installed deep-translator-1.11.4\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n# data processing ihqid-1mg\ndf=pd.read_csv('/kaggle/input/ihqid-1mg/train.csv')\ndf3=pd.read_csv('/kaggle/input/ihqid-1mg1/test.csv')\n# extract only the needed columns\n#df1=df[['question_hindi','disease_hindi','drug_hindi','treatment_hindi']]\n#df2=df3[['question_hindi','disease_hindi','drug_hindi','treatment_hindi']]\n# drop the datapoints having the null in all of its intent is null\n#df1=df1.dropna(subset=['disease_hindi','drug_hindi','treatment_hindi'],how='all')\n#df2=df2.dropna(subset=['disease_hindi','drug_hindi','treatment_hindi'],how='all')\n# create a column name intent to label the trainig point\n#df1['Manual_Intent'] = df1[['disease_hindi','drug_hindi','treatment_hindi']].notna().idxmax(axis=1)\n#df2['intent'] = df2[['disease_hindi','drug_hindi','treatment_hindi']].notna().idxmax(axis=1)\n# find the feature and lable as train and test data\ny_train=df['Manual_Intent']\nX_train=df['question_hindi']\ny_test=df3['Manual_Intent']\nX_test=df3['question_hindi']\nfrom deep_translator import GoogleTranslator\n\ngoogle_translator = GoogleTranslator(source='hi', target='en')\n\ndef translate_to_english(text):\n    translated = google_translator.translate(text)\n    return translated","metadata":{"execution":{"iopub.status.busy":"2023-12-09T06:04:08.236443Z","iopub.execute_input":"2023-12-09T06:04:08.239029Z","iopub.status.idle":"2023-12-09T06:04:08.279523Z","shell.execute_reply.started":"2023-12-09T06:04:08.238964Z","shell.execute_reply":"2023-12-09T06:04:08.278387Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# perform back translation\ny_train=list(map(translate_to_english,y_train))\nX_train=list(map(translate_to_english,X_train))\ny_test=list(map(translate_to_english,y_test))\nX_test=list(map(translate_to_english,X_test))","metadata":{"execution":{"iopub.status.busy":"2023-12-09T06:04:09.379218Z","iopub.execute_input":"2023-12-09T06:04:09.379686Z","iopub.status.idle":"2023-12-09T06:10:13.464216Z","shell.execute_reply.started":"2023-12-09T06:04:09.379648Z","shell.execute_reply":"2023-12-09T06:10:13.462797Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Trainer, TrainingArguments\n\n\n# Encode the intent labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y_train)\ny_t_encoded = label_encoder.fit_transform(y_test)\n\n# Load the RoBERTa tokenizer and model\nmodel_name = \"emilyalsentzer/Bio_ClinicalBERT\"  # You can choose a specific RoBERTa model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n\n# Tokenize and encode the training and testing data\nclass IntentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.encodings = tokenizer(\n            texts,\n            truncation=True,\n            padding='max_length',\n            max_length=max_length,\n            return_tensors='pt'\n        )\n        self.labels = torch.tensor(labels)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = self.labels[idx]\n        return item\n#print(y_train)\ntrain_dataset = IntentDataset(X_train, y_encoded, tokenizer, max_length=64)\ntest_dataset = IntentDataset(X_test, y_t_encoded, tokenizer, max_length=64)\n\n# Define training arguments and a trainer\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"steps\",\n    eval_steps=40,\n    output_dir=\"/kaggle/working\",\n    num_train_epochs=3,\n    load_best_model_at_end=True,\n    save_steps=1000,\n    save_total_limit=2,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(-1) == p.label_ids).mean()},\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Evaluate the fine-tuned model on the test set\nresults = trainer.evaluate()\n\n# Print the evaluation results\nprint(results)\n\n# Generate predictions for the test set\ny_pred_intent = trainer.predict(test_dataset).predictions.argmax(axis=1)\n\n# Decode the encoded labels\ny_pred_intent_decoded = label_encoder.inverse_transform(y_pred_intent)\n# Print the classification report\nreport = classification_report(y_test, y_pred_intent_decoded, target_names=label_encoder.classes_)\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T06:11:39.700009Z","iopub.execute_input":"2023-12-09T06:11:39.700748Z","iopub.status.idle":"2023-12-09T06:17:41.985004Z","shell.execute_reply.started":"2023-12-09T06:11:39.700698Z","shell.execute_reply":"2023-12-09T06:17:41.983634Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='117' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [117/117 05:34, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>40</td>\n      <td>No log</td>\n      <td>0.860281</td>\n      <td>0.651786</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>No log</td>\n      <td>0.705542</td>\n      <td>0.687500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.6349409818649292, 'eval_accuracy': 0.7946428571428571, 'eval_runtime': 10.5842, 'eval_samples_per_second': 10.582, 'eval_steps_per_second': 1.323, 'epoch': 3.0}\n              precision    recall  f1-score   support\n\n     disease       0.59      0.96      0.73        24\n       drugs       0.92      1.00      0.96        54\n       other       0.00      0.00      0.00        13\n   treatment       0.86      0.57      0.69        21\n\n    accuracy                           0.79       112\n   macro avg       0.59      0.63      0.59       112\nweighted avg       0.73      0.79      0.75       112\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Intent Reconginition on ihqid_webmd Dataset by translating in to english","metadata":{}},{"cell_type":"code","source":"# data processing ihqid-webmd\ndf=pd.read_csv('/kaggle/input/ihqid-webmd/train.csv')\ndf3=pd.read_csv('/kaggle/input/ihqid-webmd/test.csv')\ny_train=df['Manual_Intent']\nX_train=df['question_hindi']\ny_test=df3['Manual_Intent']\nX_test=df3['question_hindi']\n\n#translate \n# perform back translation\ny_train=list(map(translate_to_english,y_train))\nX_train=list(map(translate_to_english,X_train))\ny_test=list(map(translate_to_english,y_test))\nX_test=list(map(translate_to_english,X_test))","metadata":{"execution":{"iopub.status.busy":"2023-12-09T06:17:41.987004Z","iopub.execute_input":"2023-12-09T06:17:41.987386Z","iopub.status.idle":"2023-12-09T06:31:34.365735Z","shell.execute_reply.started":"2023-12-09T06:17:41.987333Z","shell.execute_reply":"2023-12-09T06:31:34.364511Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Trainer, TrainingArguments\n\n\n# Encode the intent labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y_train)\ny_t_encoded = label_encoder.fit_transform(y_test)\n\n# Load the RoBERTa tokenizer and model\nmodel_name = \"emilyalsentzer/Bio_ClinicalBERT\"  # You can choose a specific RoBERTa model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n\n# Tokenize and encode the training and testing data\nclass IntentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.encodings = tokenizer(\n            texts,\n            truncation=True,\n            padding='max_length',\n            max_length=max_length,\n            return_tensors='pt'\n        )\n        self.labels = torch.tensor(labels)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = self.labels[idx]\n        return item\n#print(y_train)\ntrain_dataset = IntentDataset(X_train, y_encoded, tokenizer, max_length=64)\ntest_dataset = IntentDataset(X_test, y_t_encoded, tokenizer, max_length=64)\n\n# Define training arguments and a trainer\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"steps\",\n    eval_steps=40,\n    output_dir=\"/kaggle/working\",\n    num_train_epochs=2,\n    load_best_model_at_end=True,\n    save_steps=1000,\n    save_total_limit=2,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(-1) == p.label_ids).mean()},\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Evaluate the fine-tuned model on the test set\nresults = trainer.evaluate()\n\n# Print the evaluation results\nprint(results)\n\n# Generate predictions for the test set\ny_pred_intent = trainer.predict(test_dataset).predictions.argmax(axis=1)\n\n# Decode the encoded labels\ny_pred_intent_decoded = label_encoder.inverse_transform(y_pred_intent)\n# Print the classification report\nreport = classification_report(y_test, y_pred_intent_decoded, target_names=label_encoder.classes_)\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T06:31:34.368595Z","iopub.execute_input":"2023-12-09T06:31:34.369996Z","iopub.status.idle":"2023-12-09T06:42:14.875956Z","shell.execute_reply.started":"2023-12-09T06:31:34.369936Z","shell.execute_reply":"2023-12-09T06:42:14.874438Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [180/180 09:50, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>40</td>\n      <td>No log</td>\n      <td>1.048576</td>\n      <td>0.663900</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>No log</td>\n      <td>0.767935</td>\n      <td>0.755187</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>No log</td>\n      <td>0.780963</td>\n      <td>0.713693</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>No log</td>\n      <td>0.707363</td>\n      <td>0.767635</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.6869829297065735, 'eval_accuracy': 0.7759336099585062, 'eval_runtime': 22.103, 'eval_samples_per_second': 10.903, 'eval_steps_per_second': 1.403, 'epoch': 2.0}\n                precision    recall  f1-score   support\n\n       disease       0.84      0.83      0.83        76\n         drugs       0.76      0.79      0.78        53\n         other       0.78      0.78      0.78        73\ntreatment plan       0.66      0.64      0.65        39\n\n      accuracy                           0.78       241\n     macro avg       0.76      0.76      0.76       241\n  weighted avg       0.78      0.78      0.78       241\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Intent Recognition in Indic language Queries Using xlm-roberta-base as pre trained model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n# data processing ihqid-1mg\ndf=pd.read_csv('/kaggle/input/ihqid-1mg/train.csv')\ndf3=pd.read_csv('/kaggle/input/ihqid-1mg1/test.csv')\ny_train=df['Manual_Intent']\nX_train=df['question_hindi']\ny_test=df3['Manual_Intent']\nX_test=df3['question_hindi']","metadata":{"execution":{"iopub.status.busy":"2023-12-09T06:42:14.879734Z","iopub.execute_input":"2023-12-09T06:42:14.880663Z","iopub.status.idle":"2023-12-09T06:42:14.952239Z","shell.execute_reply.started":"2023-12-09T06:42:14.880617Z","shell.execute_reply":"2023-12-09T06:42:14.950948Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Trainer, TrainingArguments\n\n\n# Encode the intent labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y_train)\ny_t_encoded = label_encoder.fit_transform(y_test)\n\n# Load the RoBERTa tokenizer and model\nmodel_name = \"xlm-roberta-base\"  # You can choose a specific RoBERTa model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n\n# Tokenize and encode the training and testing data\nclass IntentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.encodings = tokenizer(\n            texts,\n            truncation=True,\n            padding='max_length',\n            max_length=max_length,\n            return_tensors='pt'\n        )\n        self.labels = torch.tensor(labels)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = self.labels[idx]\n        return item\nprint(y_train)\ntrain_dataset = IntentDataset(X_train.tolist(), y_encoded, tokenizer, max_length=64)\ntest_dataset = IntentDataset(X_test.tolist(), y_t_encoded, tokenizer, max_length=64)\n\n# Define training arguments and a trainer\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"steps\",\n    eval_steps=37,\n    output_dir=\"/kaggle/working\",\n    num_train_epochs=8,\n    load_best_model_at_end=True,\n    save_steps=740,\n    save_total_limit=2,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(-1) == p.label_ids).mean()},\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Evaluate the fine-tuned model on the test set\nresults = trainer.evaluate()\n\n# Print the evaluation results\nprint(results)\n\n# Generate predictions for the test set\ny_pred_intent = trainer.predict(test_dataset).predictions.argmax(axis=1)\n\n# Decode the encoded labels\ny_pred_intent_decoded = label_encoder.inverse_transform(y_pred_intent)\n# Print the classification report\nreport = classification_report(y_test, y_pred_intent_decoded, target_names=label_encoder.classes_)\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T06:42:14.953644Z","iopub.execute_input":"2023-12-09T06:42:14.953986Z","iopub.status.idle":"2023-12-09T07:09:12.400457Z","shell.execute_reply.started":"2023-12-09T06:42:14.953958Z","shell.execute_reply":"2023-12-09T07:09:12.399145Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"118cd82e0fbc494c9882db41c9a00a6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c926902b7b6f464d89e9bdc942d3e2d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f79135faff4539b0605d0a5842f991"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d3a8312c8584f74909c42d7d7f7da02"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"0           drug\n1           drug\n2          other\n3           drug\n4        disease\n         ...    \n300         drug\n301    treatment\n302        other\n303        other\n304    treatment\nName: Manual_Intent, Length: 305, dtype: object\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [312/312 25:54, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>37</td>\n      <td>No log</td>\n      <td>1.115316</td>\n      <td>0.482143</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>No log</td>\n      <td>1.018675</td>\n      <td>0.616071</td>\n    </tr>\n    <tr>\n      <td>111</td>\n      <td>No log</td>\n      <td>0.976539</td>\n      <td>0.616071</td>\n    </tr>\n    <tr>\n      <td>148</td>\n      <td>No log</td>\n      <td>0.879504</td>\n      <td>0.642857</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>No log</td>\n      <td>0.892882</td>\n      <td>0.669643</td>\n    </tr>\n    <tr>\n      <td>222</td>\n      <td>No log</td>\n      <td>0.798559</td>\n      <td>0.696429</td>\n    </tr>\n    <tr>\n      <td>259</td>\n      <td>No log</td>\n      <td>1.068658</td>\n      <td>0.642857</td>\n    </tr>\n    <tr>\n      <td>296</td>\n      <td>No log</td>\n      <td>0.821479</td>\n      <td>0.696429</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.8134545683860779, 'eval_accuracy': 0.6964285714285714, 'eval_runtime': 10.6515, 'eval_samples_per_second': 10.515, 'eval_steps_per_second': 1.314, 'epoch': 8.0}\n              precision    recall  f1-score   support\n\n     disease       0.46      0.92      0.61        24\n        drug       0.88      0.98      0.93        54\n       other       0.00      0.00      0.00        13\n   treatment       0.75      0.14      0.24        21\n\n    accuracy                           0.70       112\n   macro avg       0.52      0.51      0.45       112\nweighted avg       0.66      0.70      0.62       112\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Intent Reconginition on ihqid_webmd Dataset on indic language multilingual pre-trained model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n# data processing ihqid-webmd\ndf1=pd.read_csv('/kaggle/input/ihqid-webmd/train.csv')\ndf2=pd.read_csv('/kaggle/input/ihqid-webmd/test.csv')\ny_train=df1['Manual_Intent']\nX_train=df1['question_hindi']\ny_test=df2['Manual_Intent']\nX_test=df2['question_hindi']","metadata":{"execution":{"iopub.status.busy":"2023-12-09T07:50:07.372759Z","iopub.execute_input":"2023-12-09T07:50:07.373248Z","iopub.status.idle":"2023-12-09T07:50:07.430414Z","shell.execute_reply.started":"2023-12-09T07:50:07.373206Z","shell.execute_reply":"2023-12-09T07:50:07.428497Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Trainer, TrainingArguments\n\n\n# Encode the intent labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y_train)\ny_t_encoded = label_encoder.fit_transform(y_test)\n\n# Load the RoBERTa tokenizer and model\nmodel_name = \"xlm-roberta-base\"  # You can choose a specific RoBERTa model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n\n# Tokenize and encode the training and testing data\nclass IntentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.encodings = tokenizer(\n            texts,\n            truncation=True,\n            padding='max_length',\n            max_length=max_length,\n            return_tensors='pt'\n        )\n        self.labels = torch.tensor(labels)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = self.labels[idx]\n        return item\nprint(y_train)\ntrain_dataset = IntentDataset(X_train.tolist(), y_encoded, tokenizer, max_length=64)\ntest_dataset = IntentDataset(X_test.tolist(), y_t_encoded, tokenizer, max_length=64)\n\n# Define training arguments and a trainer\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"steps\",\n    eval_steps=90,\n    output_dir=\"/kaggle/working\",\n    num_train_epochs=8,\n    load_best_model_at_end=True,\n    save_steps=900,\n    save_total_limit=2,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(-1) == p.label_ids).mean()},\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Evaluate the fine-tuned model on the test set\nresults = trainer.evaluate()\n\n# Print the evaluation results\nprint(results)\n\n# Generate predictions for the test set\ny_pred_intent = trainer.predict(test_dataset).predictions.argmax(axis=1)\n\n# Decode the encoded labels\ny_pred_intent_decoded = label_encoder.inverse_transform(y_pred_intent)\n# Print the classification report\nreport = classification_report(y_test, y_pred_intent_decoded, target_names=label_encoder.classes_)\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T07:50:08.920615Z","iopub.execute_input":"2023-12-09T07:50:08.921058Z","iopub.status.idle":"2023-12-09T08:50:04.288117Z","shell.execute_reply.started":"2023-12-09T07:50:08.921023Z","shell.execute_reply":"2023-12-09T08:50:04.287082Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"0                drug\n1               other\n2                drug\n3             disease\n4             disease\n            ...      \n715             other\n716    treatment plan\n717    treatment plan\n718    treatment plan\n719    treatment plan\nName: Manual_Intent, Length: 720, dtype: object\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='720' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [720/720 58:54, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>90</td>\n      <td>No log</td>\n      <td>1.357756</td>\n      <td>0.315353</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>No log</td>\n      <td>1.234995</td>\n      <td>0.423237</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>No log</td>\n      <td>1.294584</td>\n      <td>0.394191</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>No log</td>\n      <td>1.198205</td>\n      <td>0.481328</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>No log</td>\n      <td>1.142437</td>\n      <td>0.497925</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>1.320300</td>\n      <td>1.097869</td>\n      <td>0.531120</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>1.320300</td>\n      <td>1.081129</td>\n      <td>0.535270</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>1.320300</td>\n      <td>1.066980</td>\n      <td>0.580913</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 1.0669803619384766, 'eval_accuracy': 0.5809128630705395, 'eval_runtime': 22.4658, 'eval_samples_per_second': 10.727, 'eval_steps_per_second': 1.38, 'epoch': 8.0}\n                precision    recall  f1-score   support\n\n       disease       0.50      0.70      0.59        76\n          drug       0.53      0.77      0.63        53\n         other       0.78      0.63      0.70        73\ntreatment plan       0.00      0.00      0.00        39\n\n      accuracy                           0.58       241\n     macro avg       0.45      0.53      0.48       241\n  weighted avg       0.51      0.58      0.53       241\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}