{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6851440,"sourceType":"datasetVersion","datasetId":3938247},{"sourceId":6851842,"sourceType":"datasetVersion","datasetId":3938468},{"sourceId":6883472,"sourceType":"datasetVersion","datasetId":3954811}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Entity Extraction and biotagging in the healthcare queries in indic language","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv('/kaggle/input/ihqid-1mg/train.csv')\ndf3=pd.read_csv('/kaggle/input/ihqid-1mg1/test.csv')\ny_train=df['Manual_Intent']\nX_train=df['question_english']\ny_test=df3['Manual_Intent']\nX_test=df3['question_english']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df1=df[['question_english','disease_english','drug_english','treatment_english']]\n#df2=df3[['question_english','disease_english','drug_english','treatment_english']]\n#print(df2.columns)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T04:58:28.005613Z","iopub.execute_input":"2023-11-05T04:58:28.006099Z","iopub.status.idle":"2023-11-05T04:58:28.028463Z","shell.execute_reply.started":"2023-11-05T04:58:28.006061Z","shell.execute_reply":"2023-11-05T04:58:28.027587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df1=df1.dropna(subset=['disease_english','drug_english','treatment_english'],how='all')\n#df2=df2.dropna(subset=['disease_english','drug_english','treatment_english'],how='all')","metadata":{"execution":{"iopub.status.busy":"2023-11-05T04:58:30.365976Z","iopub.execute_input":"2023-11-05T04:58:30.366672Z","iopub.status.idle":"2023-11-05T04:58:30.377462Z","shell.execute_reply.started":"2023-11-05T04:58:30.366633Z","shell.execute_reply":"2023-11-05T04:58:30.375967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df1['intent'] = df1[['disease_english','drug_english','treatment_english']].notna().idxmax(axis=1)\n#df2['intent'] = df2[['disease_english','drug_english','treatment_english']].notna().idxmax(axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T04:58:31.995552Z","iopub.execute_input":"2023-11-05T04:58:31.995989Z","iopub.status.idle":"2023-11-05T04:58:32.013322Z","shell.execute_reply.started":"2023-11-05T04:58:31.995953Z","shell.execute_reply":"2023-11-05T04:58:32.012017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install fuzzywuzzy python-Levenshtein deep-translator","metadata":{"execution":{"iopub.status.busy":"2023-11-11T13:28:21.224556Z","iopub.execute_input":"2023-11-11T13:28:21.224942Z","iopub.status.idle":"2023-11-11T13:28:33.284448Z","shell.execute_reply.started":"2023-11-11T13:28:21.224912Z","shell.execute_reply":"2023-11-11T13:28:33.282952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Trainer, TrainingArguments\n\n\n# Encode the intent labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y_train)\ny_t_encoded = label_encoder.fit_transform(y_test)\n\n# Load the RoBERTa tokenizer and model\nmodel_name = \"roberta-base\"  # You can choose a specific RoBERTa model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n\n# Tokenize and encode the training and testing data\nclass IntentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.encodings = tokenizer(\n            texts,\n            truncation=True,\n            padding='max_length',\n            max_length=max_length,\n            return_tensors='pt'\n        )\n        self.labels = torch.tensor(labels)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = self.labels[idx]\n        return item\ntrain_dataset = IntentDataset(X_train.tolist(), y_encoded, tokenizer, max_length=64)\ntest_dataset = IntentDataset(X_test.tolist(), y_t_encoded, tokenizer, max_length=64)\n\n# Define training arguments and a trainer\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"steps\",\n    eval_steps=60,\n    output_dir=\"/kaggle/working\",\n    num_train_epochs=3,\n    load_best_model_at_end=True,\n    save_steps=600,\n    save_total_limit=2,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(-1) == p.label_ids).mean()},\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Evaluate the fine-tuned model on the test set\nresults = trainer.evaluate()\n\n# Print the evaluation results\nprint(results)\n\n# Generate predictions for the test set\ny_pred_intent = trainer.predict(test_dataset).predictions.argmax(axis=1)\n\n# Decode the encoded labels\ny_pred_intent_decoded = label_encoder.inverse_transform(y_pred_intent)\n# Print the classification report\nreport = classification_report(y_test, y_pred_intent_decoded, target_names=label_encoder.classes_)\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-10T09:19:28.858326Z","iopub.execute_input":"2023-11-10T09:19:28.858752Z","iopub.status.idle":"2023-11-10T09:44:45.995015Z","shell.execute_reply.started":"2023-11-10T09:19:28.858721Z","shell.execute_reply":"2023-11-10T09:44:45.994050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"webmd_train=pd.read_csv('/kaggle/input/ihqid-webmd/train.csv')\nwebmd_test=pd.read_csv('/kaggle/input/ihqid-webmd/test.csv')\n# extract only reuired english columns\n#df1=webmd_train[['question_english','disease_english','drug_english','treatment_english']]\n#df2=webmd_test[['question_english','disease_english','drug_english','treatment_english']]\n#df1['intent'] = df1[['disease_english','drug_english','treatment_english']].notna().idxmax(axis=1)\n#df2['intent'] = df2[['disease_english','drug_english','treatment_english']].notna().idxmax(axis=1)\ny_train=webmd_train['Manual_Intent']\nX_train=webmd_train['question_english']\ny_test=webmd_test['Manual_Intent']\nX_test=webmd_test['question_english']","metadata":{"execution":{"iopub.status.busy":"2023-11-10T13:05:37.442434Z","iopub.execute_input":"2023-11-10T13:05:37.442888Z","iopub.status.idle":"2023-11-10T13:05:37.542404Z","shell.execute_reply.started":"2023-11-10T13:05:37.442856Z","shell.execute_reply":"2023-11-10T13:05:37.541063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Trainer, TrainingArguments\n\n\n# Encode the intent labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y_train)\ny_t_encoded = label_encoder.fit_transform(y_test)\n\n# Load the RoBERTa tokenizer and model\nmodel_name = \"roberta-base\"  # You can choose a specific RoBERTa model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n\n# Tokenize and encode the training and testing data\nclass IntentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.encodings = tokenizer(\n            texts,\n            truncation=True,\n            padding='max_length',\n            max_length=max_length,\n            return_tensors='pt'\n        )\n        self.labels = torch.tensor(labels)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = self.labels[idx]\n        return item\ntrain_dataset = IntentDataset(X_train.tolist(), y_encoded, tokenizer, max_length=64)\ntest_dataset = IntentDataset(X_test.tolist(), y_t_encoded, tokenizer, max_length=64)\n\n# Define training arguments and a trainer\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"steps\",\n    eval_steps=90,\n    output_dir=\"/kaggle/working\",\n    num_train_epochs=2,\n    load_best_model_at_end=True,\n    save_steps=900,\n    save_total_limit=2,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(-1) == p.label_ids).mean()},\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Evaluate the fine-tuned model on the test set\nresults = trainer.evaluate()\n\n# Print the evaluation results\nprint(results)\n\n# Generate predictions for the test set\ny_pred_intent = trainer.predict(test_dataset).predictions.argmax(axis=1)\n\n# Decode the encoded labels\ny_pred_intent_decoded = label_encoder.inverse_transform(y_pred_intent)\n# Print the classification report\nreport = classification_report(y_test, y_pred_intent_decoded, target_names=label_encoder.classes_)\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-10T13:32:15.757948Z","iopub.execute_input":"2023-11-10T13:32:15.758452Z","iopub.status.idle":"2023-11-10T13:43:47.233923Z","shell.execute_reply.started":"2023-11-10T13:32:15.758417Z","shell.execute_reply":"2023-11-10T13:43:47.232608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}